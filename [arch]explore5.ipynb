{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                 product_name product_price  \\\n",
      "0           0  FurReal Dazzlin' Dimples My Playful Dolphin        £39.99   \n",
      "1           1          Tiffany's World Day Out At The Park        £12.99   \n",
      "2           2          Tiffany's World Pups Picnic Playset         £7.00   \n",
      "3           3     Tiffany's World Wildlife Park Adventures        £12.99   \n",
      "4           4                      Cosatto Cosy Dolls Pram        £30.00   \n",
      "\n",
      "   weight        category            EAN  date_added  \\\n",
      "0   1.6kg  toys-and-games  7425710935115  2005-12-02   \n",
      "1  0.48kg  toys-and-games   487128731892  2006-01-09   \n",
      "2    590g  toys-and-games  1945816904649  1997-03-29   \n",
      "3    540g  toys-and-games  1569790890899  2013-03-20   \n",
      "4  1.91kg  toys-and-games  7142740213920  2007-12-23   \n",
      "\n",
      "                                   uuid          removed product_code  \n",
      "0  83dc0a69-f96f-4c34-bcb7-928acae19a94  Still_avaliable  R7-3126933h  \n",
      "1  712254d7-aea7-4310-aff8-8bcdd0aec7ff  Still_avaliable  C2-7287916l  \n",
      "2  b089ef6f-b628-4e37-811d-fffe0102ba64  Still_avaliable  S7-1175877v  \n",
      "3  d55de422-8b98-47d6-9991-e4bc4c5c0cb0          Removed  D8-8421505n  \n",
      "4  7945b657-cb02-4cc5-96cf-f65ed0a8f235  Still_avaliable  B6-2596063a  \n"
     ]
    }
   ],
   "source": [
    "from data_extraction import DataExtractor\n",
    "\n",
    "# Create an instance of the DataExtractor\n",
    "extractor = DataExtractor()\n",
    "\n",
    "# Call the method\n",
    "s3_uri = 's3://data-handling-public/products.csv'\n",
    "df = extractor.extract_from_s3(s3_uri)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     1.6kg\n",
      "1    0.48kg\n",
      "2      590g\n",
      "3      540g\n",
      "4    1.91kg\n",
      "Name: weight, dtype: object\n",
      "['1.6kg' '0.48kg' '590g' '540g' '1.91kg' '0.91kg' '0.46kg' '0.38kg'\n",
      " '8.981kg' '1.478kg' '1.2g' '0.66kg' '1.8kg' '1.9kg' '1.725kg' '0.54kg'\n",
      " '0.322kg' '0.71kg' '0.88kg' '0.67kg' '11.076kg' '4kg' '0.385kg' '1.38kg'\n",
      " '2.57kg' '1.35kg' '0.695kg' '1.15kg' '0.98kg' '1.447kg' '1.3625kg'\n",
      " '2.25kg' '0.79kg' '0.8kg' '1.08kg' '2.476kg' '0.137kg' '11.5kg' '0.44kg'\n",
      " '2.75kg' '0.911kg' '0.33kg' '1kg' '0.5kg' '0.45kg' '0.7kg' '0.41kg'\n",
      " '1.3kg' '2kg' '0.34kg' '0.37kg' '0.76kg' '1.18kg' '0.685kg' '1.59kg'\n",
      " '1.4kg' '1.66kg' '13.5kg' '0.745kg' '1.44kg' '0.74kg' '0.660kg' '0.419kg'\n",
      " '0.418kg' '0.470kg' '0.353kg' '0.350kg' '0.96kg' '1.20kg' '1.21kg'\n",
      " '1.02kg' '0.365kg' '0.677kg' '0.55kg' '0.43kg' '0.11kg' '1.23kg' '1.03kg'\n",
      " '0.87kg' '0.39kg' '0.35kg' '0.42kg' '0.27kg' '726g' '0.61kg' '0.864kg'\n",
      " '0.667kg' '0.63kg' '0.72kg' '0.58kg' '0.627kg' '0.3kg' '0.32kg' '0.01kg'\n",
      " '0.650kg' '0.68kg' '0.36kg' '1.395kg' '0.9kg' '0.468kg' '0.687kg'\n",
      " '0.955kg' '0.700kg' '0.900kg' '0.06kg' '0.967kg' '0.03kg' '1.041kg'\n",
      " '0.6kg' '0.65kg' '0.59kg' '0.639kg' '0.649kg' '0.354kg' '1.25kg' '0.16kg'\n",
      " '0.258kg' '0.313kg' '0.08kg' '420g' '1.68kg' '0.718kg' '0.92kg' '0.072kg'\n",
      " '1.7kg' '3kg' '1.2kg' '1.505kg' '1.49kg' '0.492kg' '3.3kg' '3.17kg'\n",
      " '2.565kg' '0.443kg' '1.75kg' '0.21kg' '0.86kg' '0.882kg' '0.4kg' '0.77kg'\n",
      " '2.2kg' '1.27kg' '0.809kg' '0.378kg' '3.1kg' '1.84kg' '0.47kg' nan\n",
      " '1.16kg' '1.1kg' '0.087kg' '0.504kg' '480g' '12 x 100g' '8 x 150g'\n",
      " '6 x 412g' '6 x 400g' '15kg' '160g' '180g' '112g' '120g' '96g' '125g'\n",
      " '110g' '128g' '115g' '170g' '200g' '80g' '130g' '140g' '53g' '500g'\n",
      " '100g' '700g' '750g' '800g' '8 x 85g' '40 x 100g' '12 x 85g' '50g' '300g'\n",
      " '512g' '5.6kg' '6.4kg' '10kg' '14.8kg' '2.9kg' '2.4kg' '6kg' '18.7kg'\n",
      " '11kg' '7.5kg' '4.1kg' '13.7kg' '3.03kg' '3.8kg' '1.071kg' '13kg' '1.5kg'\n",
      " '2.8kg' '6.2kg' '296g' '1.65kg' '1.06kg' '2290g' '1950g' '1675g' '1550g'\n",
      " '1650g' '1450g' '1300g' '1100g' '2200g' '1850g' '1800g' '1500g' '250g'\n",
      " '0.2kg' '1.55kg' '1.40kg' '2100g' '1750g' '0.29kg' '450g' '10.5g' '280g'\n",
      " '230g' '150g' '416g' '163g' '360g' '390g' '1470g' '520g' '270g' '60g'\n",
      " '380g' '350g' '282g' '155g' '2.06kg' '3.2kg' '2.5kg' '10.5kg' '4.3kg'\n",
      " '9kg' '31kg' '12.8kg' '32.1kg' '16.7kg' '23.9kg' '20kg' '11.85kg'\n",
      " '18.5kg' '33kg' '8.2kg' '970g' '2.280kg' '1.280kg' '1.200kg' '1.440kg'\n",
      " '550g' '30g' '1230g' '52g' '45g' '650g' '340g' '265g' '1.09kg' '14g'\n",
      " '998g' '335g' '28kg' '6.25kg' '9.3kg' '28.8kg' '35.5kg' '537g' '375g'\n",
      " '260g' '600g' '61g' '165g' '475g' '26g' '615g' '582g' '430g' '440g'\n",
      " '0.423kg' '0.126kg' '0.22kg' '190g' '370g' '470g' '368g' '410g' '4.6kg'\n",
      " '3.4kg' '2.05kg' '2.1kg' '1.76kg' '2.85kg' '2.36kg' '1.37kg' '1.72kg'\n",
      " '1.73kg' '1.63kg' '0.75kg' '4.53kg' '3.45kg' '2.03kg' '9GO9NZ5JTL'\n",
      " '4.5kg' '12.1kg' '9.4kg' '6.1kg' '4.2kg' '12kg' '21kg' '7.8kg' '5.7kg'\n",
      " '5kg' '1760g' '3.75kg' '4.85kg' '5.5kg' '11.6kg' '10.7kg' '22.5kg'\n",
      " '19.8kg' '8.5kg' '15.5kg' '2.7kg' '27kg' '9.8kg' '12.5kg' '7.3kg' '8.4kg'\n",
      " '5.4kg' '15.8kg' '3.9kg' '6.3kg' '7kg' '6.9kg' '11.2kg' '12.3kg' '11.3kg'\n",
      " '15.6kg' '2.40kg' '2.70kg' '1.32kg' '1.80kg' '2.4g' '1.38g' '3.65kg'\n",
      " '1020g' '1220g' '185g' '887g' '570g' '365g' '0.24kg' '0.51kg' '0.25kg'\n",
      " '0.26kg' '0.81kg' '220g' '310g' '490g' '0.202kg' '630g' 'Z8ZTDGUZVU'\n",
      " '330g' '850g' '936g' '363g' '240g' '856g' '169g' '3.429kg' '2.770kg'\n",
      " '1.870kg' '2.150kg' '1.250kg' '1510g' '1250g' '1700g' '2050g' '1780g'\n",
      " '795g' '320g' '0.28kg' '226g' 'MX180RYSHX' '422g' '184g' '580g' '245g'\n",
      " '7.4kg' '1320g' '1090g' '1.56kg' '1160kg' '1060g' '1580g' '1365g'\n",
      " '2.14kg' '1.782kg' '1.081kg' '0.964kg' '2.66kg' '3.028g' '0.944g' '1.5g'\n",
      " '1.507g' '5.25kg' '3.82kg' '1.95kg' '1.05kg' '1.74kg' '0.89kg' '2.29kg'\n",
      " '810g' '3.85kg' '70g' '100ml' '3 x 2g' '4g' '60ml' '8g' '3 x 90g' '87g'\n",
      " '46g' '95g' '43g' '324g' '152g' '141g' '94g' '351g' '92g' '132g' '105g'\n",
      " '16 x 10g' '425g' '3 x 132g' '5 x 145g' '4 x 400g' '206g' '465g' '395g'\n",
      " '300ml' '800ml' '225g' '167g' '131g' '2 x 200g' '391g' '400g' '275g'\n",
      " '195g' '285g' '248g' '18g' '90g' '216g' '201g' '77g .' '99g' '114g'\n",
      " '0.115kg' '0.196kg' '290g' '3g' '16oz' '274g' '289g' '1.28kg' '820g'\n",
      " '134g']\n"
     ]
    }
   ],
   "source": [
    "# Display the first few entries in the weight column to understand its current format\n",
    "print(df['weight'].head())\n",
    "\n",
    "# Get a broader overview by looking at unique values\n",
    "unique_weights = df['weight'].unique()\n",
    "print(unique_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     weight number    unit\n",
      "0     1.5kg    1.5      kg\n",
      "1     300 g    300       g\n",
      "2     200ml    200      ml\n",
      "3  2 liters      2  liters\n",
      "4         ?   None    None\n",
      "5  500grams    500   grams\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Define a function to extract numbers and units\n",
    "def extract_number_unit(s):\n",
    "    if pd.isna(s):\n",
    "        return [None, None]\n",
    "    # Use a raw string for the regular expression\n",
    "    parts = re.split(r'(\\d+\\.?\\d*)', s.replace(' ', ''))\n",
    "    if len(parts) >= 3:\n",
    "        return [parts[1], ''.join(parts[2:])]\n",
    "    return [None, None]\n",
    "\n",
    "# Example usage with DataFrame\n",
    "data = {\n",
    "    'weight': ['1.5kg', '300 g', '200ml', '2 liters', '?', '500grams']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply this function to the 'weight' column\n",
    "df['weight_split'] = df['weight'].apply(extract_number_unit)\n",
    "df[['number', 'unit']] = pd.DataFrame(df['weight_split'].tolist(), index=df.index)\n",
    "\n",
    "# Print the DataFrame to see the extracted parts\n",
    "print(df[['weight', 'number', 'unit']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_units(unit):\n",
    "    if unit is None:  # Check if the unit is None and return None immediately if true\n",
    "        return None\n",
    "    unit = unit.lower().strip()  # Safely apply string methods now\n",
    "    if 'kg' in unit or 'kilogram' in unit:\n",
    "        return 'kg'\n",
    "    elif 'g' in unit or 'gram' in unit:\n",
    "        return 'g'\n",
    "    elif 'ml' in unit or 'milliliter' in unit:\n",
    "        return 'g'  # treating ml as grams as per the 1:1 volume to mass ratio given\n",
    "    elif 'liter' in unit:\n",
    "        return 'liters'  # you might need to decide how to handle liters if they need conversion\n",
    "    else:\n",
    "        return None  # for handling unknown or unparsable units\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the adjusted function\n",
    "df_copy['unit'] = df_copy['unit'].apply(normalize_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kg' 'g' None]\n"
     ]
    }
   ],
   "source": [
    "print(df_copy['unit'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_units(unit):\n",
    "    if unit is None:  # Check if the unit is None and return None immediately if true\n",
    "        return None\n",
    "    unit = unit.lower().strip()  # Safely apply string methods now\n",
    "    if 'kg' in unit or 'kilogram' in unit:\n",
    "        return 'kg'\n",
    "    elif 'g' in unit or 'gram' in unit:\n",
    "        return 'g'\n",
    "    elif 'ml' in unit or 'milliliter' in unit:\n",
    "        return 'g'  # treating ml as grams as per the 1:1 volume to mass ratio given\n",
    "    elif 'liter' in unit or 'liters' in unit:\n",
    "        return 'liters'  # Now correctly handling both 'liter' and 'liters'\n",
    "    else:\n",
    "        return None  # for handling unknown or unparsable units\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kg' 'g' 'liters' None]\n"
     ]
    }
   ],
   "source": [
    "# Create a second copy of the original DataFrame to ensure no carry-over mistakes\n",
    "df_second_copy = df.copy()\n",
    "\n",
    "# Apply the corrected normalization function\n",
    "df_second_copy['unit'] = df_second_copy['unit'].apply(normalize_units)\n",
    "\n",
    "# Check the results\n",
    "print(df_second_copy['unit'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Apply the function to create the weight_kg column\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m df_final[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_kg\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_final\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Optionally, drop the original 'weight' column if no longer needed\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# df_final.drop('weight', axis=1, inplace=True)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Display the first few rows to see the final DataFrame\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_final[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_kg\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[0;32m~/Documents/AICore - Data Engineering/Data_Centralisation/venv/lib/python3.12/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/AICore - Data Engineering/Data_Centralisation/venv/lib/python3.12/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/AICore - Data Engineering/Data_Centralisation/venv/lib/python3.12/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/Documents/AICore - Data Engineering/Data_Centralisation/venv/lib/python3.12/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/AICore - Data Engineering/Data_Centralisation/venv/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m, in \u001b[0;36mprocess_weight\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_weight\u001b[39m(row):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39misna(\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m):\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Extract number and unit using regex\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Assuming 'df' is your original DataFrame\n",
    "# Create another new copy of the original DataFrame\n",
    "df_final = df.copy()\n",
    "\n",
    "# Function to normalize units and convert weights to kg\n",
    "def process_weight(row):\n",
    "    if pd.isna(row['weight']):\n",
    "        return None\n",
    "    # Extract number and unit using regex\n",
    "    parts = re.split(r'(\\d+\\.?\\d*)', row['weight'].replace(' ', ''))\n",
    "    if len(parts) < 3:\n",
    "        return None\n",
    "    number = parts[1]\n",
    "    unit = parts[2].lower().strip()\n",
    "    \n",
    "    # Normalize units\n",
    "    if 'kg' in unit or 'kilogram' in unit:\n",
    "        unit = 'kg'\n",
    "    elif 'g' in unit or 'gram' in unit:\n",
    "        unit = 'g'\n",
    "    elif 'ml' in unit or 'milliliter' in unit or 'millilitre' in unit:\n",
    "        unit = 'g'  # Treating ml as grams\n",
    "    elif 'liter' in unit or 'liters' in unit or 'litre' in unit or 'litres' in unit:\n",
    "        unit = 'liters'  # This requires decision on conversion\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    # Convert number to float\n",
    "    try:\n",
    "        number = float(number)\n",
    "        if unit == 'g':\n",
    "            return number / 1000  # Convert grams to kilograms\n",
    "        elif unit == 'kg':\n",
    "            return number  # Already in kilograms\n",
    "        elif unit == 'liters':\n",
    "            # Assuming 1 liter of water = 1 kg\n",
    "            return number\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "# Apply the function to create the weight_kg column\n",
    "df_final['weight_kg'] = df_final['weight'].apply(process_weight)\n",
    "\n",
    "# Optionally, drop the original 'weight' column if no longer needed\n",
    "# df_final.drop('weight', axis=1, inplace=True)\n",
    "\n",
    "# Display the first few rows to see the final DataFrame\n",
    "print(df_final[['weight', 'weight_kg']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_weight(row):\n",
    "    if pd.isna(row['weight']):\n",
    "        return None\n",
    "    # Extract number and unit using regex\n",
    "    parts = re.split(r'(\\d+\\.?\\d*)', row['weight'].replace(' ', ''))\n",
    "    if len(parts) < 3:  # Ensure there are enough parts to avoid indexing errors\n",
    "        return None\n",
    "    number = parts[1]\n",
    "    unit = ''.join(parts[2:]).lower().strip()  # Join all parts beyond the first split to form the unit string\n",
    "    \n",
    "    # Normalize units\n",
    "    if 'kg' in unit or 'kilogram' in unit:\n",
    "        unit = 'kg'\n",
    "    elif 'g' in unit or 'gram' in unit:\n",
    "        unit = 'g'\n",
    "    elif 'ml' in unit or 'milliliter' in unit or 'millilitre' in unit:\n",
    "        unit = 'g'  # Treating ml as grams\n",
    "    elif 'liter' in unit or 'liters' in unit or 'litre' in unit or 'litres' in unit:\n",
    "        unit = 'liters'  # Correctly handling all variations of 'liter'\n",
    "    else:\n",
    "        return None  # Handle unknown units\n",
    "    \n",
    "    # Convert number to float and calculate the weight in kilograms\n",
    "    try:\n",
    "        number = float(number)\n",
    "        if unit == 'g':\n",
    "            return number / 1000  # Convert grams to kilograms\n",
    "        elif unit == 'kg':\n",
    "            return number  # Already in kilograms\n",
    "        elif unit == 'liters':\n",
    "            # Assuming 1 liter of water = 1 kg\n",
    "            return number\n",
    "    except ValueError:\n",
    "        return None  # Handle any conversion issues or non-numeric input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Apply the updated function to create the weight_kg column\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_final[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_kg\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_final\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Display the first few rows to see the final DataFrame\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_final[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_kg\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[0;32m~/Documents/AICore - Data Engineering/Data_Centralisation/venv/lib/python3.12/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/AICore - Data Engineering/Data_Centralisation/venv/lib/python3.12/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/AICore - Data Engineering/Data_Centralisation/venv/lib/python3.12/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/Documents/AICore - Data Engineering/Data_Centralisation/venv/lib/python3.12/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/AICore - Data Engineering/Data_Centralisation/venv/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m, in \u001b[0;36mprocess_weight\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_weight\u001b[39m(row):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39misna(\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Extract number and unit using regex\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "# Apply the updated function to create the weight_kg column\n",
    "df_final['weight_kg'] = df_final['weight'].apply(process_weight)\n",
    "\n",
    "# Display the first few rows to see the final DataFrame\n",
    "print(df_final[['weight', 'weight_kg']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new copy of the original DataFrame\n",
    "df_final = df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define a function to extract number and unit\n",
    "def extract_number_unit(s):\n",
    "    if pd.isna(s):\n",
    "        return [None, None]\n",
    "    parts = re.split(r'(\\d+\\.?\\d*)', s.replace(' ', ''))\n",
    "    if len(parts) < 3:\n",
    "        return [None, None]\n",
    "    number = parts[1]\n",
    "    unit = ''.join(parts[2:]).lower().strip()\n",
    "    return [number, unit]\n",
    "\n",
    "# Apply the function to create separate number and unit columns\n",
    "df_final[['number', 'unit']] = df_final['weight'].apply(extract_number_unit).apply(pd.Series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize units\n",
    "def normalize_units(unit):\n",
    "    if unit is None:\n",
    "        return None\n",
    "    unit = unit.lower().strip()\n",
    "    if 'kg' in unit or 'kilogram' in unit:\n",
    "        return 'kg'\n",
    "    elif 'g' in unit or 'gram' in unit:\n",
    "        return 'g'\n",
    "    elif 'ml' in unit or 'milliliter' in unit or 'millilitre' in unit:\n",
    "        return 'g'\n",
    "    elif 'liter' in unit or 'liters' in unit or 'litre' in unit or 'litres' in unit:\n",
    "        return 'liters'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply the normalization function\n",
    "df_final['unit'] = df_final['unit'].apply(normalize_units)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert weights to kilograms\n",
    "def convert_to_kilograms(row):\n",
    "    try:\n",
    "        number = float(row['number'])\n",
    "        unit = row['unit']\n",
    "        if unit == 'g':\n",
    "            return number / 1000  # Convert grams to kilograms\n",
    "        elif unit == 'kg':\n",
    "            return number  # Already in kilograms\n",
    "        elif unit == 'liters':\n",
    "            return number  # Assuming 1 liter = 1 kg\n",
    "        else:\n",
    "            return None\n",
    "    except (TypeError, ValueError):\n",
    "        return None\n",
    "\n",
    "# Apply the conversion function\n",
    "df_final['weight_kg'] = df_final.apply(convert_to_kilograms, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any rows where weight_kg could not be calculated\n",
    "df_final = df_final[df_final['weight_kg'].notna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original weight, number, and unit columns if no longer needed\n",
    "df_final.drop(['weight', 'number', 'unit'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   weight_split  weight_kg\n",
      "0     [1.5, kg]        1.5\n",
      "1      [300, g]        0.3\n",
      "2     [200, ml]        0.2\n",
      "3   [2, liters]        2.0\n",
      "5  [500, grams]        0.5\n"
     ]
    }
   ],
   "source": [
    "# Rename the weight column to weight_kg\n",
    "# This step is already achieved during the conversion, so no additional renaming is required.\n",
    "\n",
    "# Display the first few rows to verify the final DataFrame\n",
    "print(df_final.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n"
     ]
    }
   ],
   "source": [
    "# Check the data type of the weight_kg column\n",
    "print(df_final['weight_kg'].dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                 product_name product_price  \\\n",
      "0           0  FurReal Dazzlin' Dimples My Playful Dolphin        £39.99   \n",
      "1           1          Tiffany's World Day Out At The Park        £12.99   \n",
      "2           2          Tiffany's World Pups Picnic Playset         £7.00   \n",
      "3           3     Tiffany's World Wildlife Park Adventures        £12.99   \n",
      "4           4                      Cosatto Cosy Dolls Pram        £30.00   \n",
      "\n",
      "   weight        category            EAN  date_added  \\\n",
      "0   1.6kg  toys-and-games  7425710935115  2005-12-02   \n",
      "1  0.48kg  toys-and-games   487128731892  2006-01-09   \n",
      "2    590g  toys-and-games  1945816904649  1997-03-29   \n",
      "3    540g  toys-and-games  1569790890899  2013-03-20   \n",
      "4  1.91kg  toys-and-games  7142740213920  2007-12-23   \n",
      "\n",
      "                                   uuid          removed product_code  \n",
      "0  83dc0a69-f96f-4c34-bcb7-928acae19a94  Still_avaliable  R7-3126933h  \n",
      "1  712254d7-aea7-4310-aff8-8bcdd0aec7ff  Still_avaliable  C2-7287916l  \n",
      "2  b089ef6f-b628-4e37-811d-fffe0102ba64  Still_avaliable  S7-1175877v  \n",
      "3  d55de422-8b98-47d6-9991-e4bc4c5c0cb0          Removed  D8-8421505n  \n",
      "4  7945b657-cb02-4cc5-96cf-f65ed0a8f235  Still_avaliable  B6-2596063a  \n"
     ]
    }
   ],
   "source": [
    "from data_extraction import DataExtractor\n",
    "\n",
    "# Create an instance of the DataExtractor\n",
    "extractor = DataExtractor()\n",
    "\n",
    "# Call the method\n",
    "s3_uri = 's3://data-handling-public/products.csv'\n",
    "df = extractor.extract_from_s3(s3_uri)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Check for special characters in product names\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m special_characters \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mproduct_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m[^a-zA-Z0-9 ]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpecial characters found in product names:\u001b[39m\u001b[38;5;124m\"\u001b[39m, special_characters\u001b[38;5;241m.\u001b[39mexplode()\u001b[38;5;241m.\u001b[39mvalue_counts())\n",
      "File \u001b[0;32m~/Documents/AICore - Data Engineering/Data_Centralisation/venv/lib/python3.12/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/AICore - Data Engineering/Data_Centralisation/venv/lib/python3.12/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/AICore - Data Engineering/Data_Centralisation/venv/lib/python3.12/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/Documents/AICore - Data Engineering/Data_Centralisation/venv/lib/python3.12/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/AICore - Data Engineering/Data_Centralisation/venv/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[39], line 2\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Check for special characters in product names\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m special_characters \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m[^a-zA-Z0-9 ]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpecial characters found in product names:\u001b[39m\u001b[38;5;124m\"\u001b[39m, special_characters\u001b[38;5;241m.\u001b[39mexplode()\u001b[38;5;241m.\u001b[39mvalue_counts())\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.12/3.12.3/Frameworks/Python.framework/Versions/3.12/lib/python3.12/re/__init__.py:217\u001b[0m, in \u001b[0;36mfindall\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfindall\u001b[39m(pattern, string, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    210\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of all non-overlapping matches in the string.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \n\u001b[1;32m    212\u001b[0m \u001b[38;5;124;03m    If one or more capturing groups are present in the pattern, return\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m \n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03m    Empty matches are included in the result.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object, got 'float'"
     ]
    }
   ],
   "source": [
    "# Check for special characters in product names\n",
    "special_characters = df['product_name'].apply(lambda x: re.findall(r'[^a-zA-Z0-9 ]', x))\n",
    "print(\"Special characters found in product names:\", special_characters.explode().value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     £39.99\n",
      "1     £12.99\n",
      "2      £7.00\n",
      "3     £12.99\n",
      "4     £30.00\n",
      "5     £12.99\n",
      "6      £7.00\n",
      "7     £12.99\n",
      "8     £12.99\n",
      "9     £89.99\n",
      "10    £24.99\n",
      "11    £24.99\n",
      "12    £24.99\n",
      "13    £12.99\n",
      "14    £12.99\n",
      "15    £22.00\n",
      "16    £22.00\n",
      "17    £20.00\n",
      "18    £16.99\n",
      "19    £12.99\n",
      "20    £39.99\n",
      "21    £12.99\n",
      "22    £12.99\n",
      "23    £12.99\n",
      "24    £25.00\n",
      "25    £45.00\n",
      "26    £24.99\n",
      "27    £12.99\n",
      "28    £18.99\n",
      "29    £12.99\n",
      "30    £29.99\n",
      "31    £12.99\n",
      "32    £12.99\n",
      "33    £15.99\n",
      "34    £30.00\n",
      "35    £12.99\n",
      "36    £12.99\n",
      "37    £19.99\n",
      "38    £12.99\n",
      "39    £12.99\n",
      "40    £12.99\n",
      "41    £25.00\n",
      "42    £45.00\n",
      "43    £24.99\n",
      "44    £12.99\n",
      "45    £18.99\n",
      "46    £12.99\n",
      "47    £29.99\n",
      "48    £12.99\n",
      "49    £12.99\n",
      "Name: product_price, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the product_price column\n",
    "print(df['product_price'].head(50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of product_price: object\n"
     ]
    }
   ],
   "source": [
    "# Check the data type of the product_price column\n",
    "print(\"Data type of product_price:\", df['product_price'].dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique product prices: ['£39.99' '£12.99' '£7.00' '£30.00' '£89.99' '£24.99' '£22.00' '£20.00'\n",
      " '£16.99' '£25.00' '£45.00' '£18.99' '£29.99' '£15.99' '£19.99' '£11.99'\n",
      " '£14.99' '£32.99' '£120.00' '£10.00' '£21.99' '£13.99' '£30.99' '£270.00'\n",
      " '£40.99' '£42.99' '£71.99' '£34.99' '£52.99' '£13.00' '£2.25' '£3.00'\n",
      " '£3.50' '£4.00' '£26.99' '£17.99' '£31.99' '£44.99' '£27.99' '£33.49'\n",
      " '£23.49' '£15.00' '£22.99' '£6.99' '£12.00' '£5.99' '£3.99' '£50.00'\n",
      " '£49.99' '£36.49' nan '£1.50' '£2.50' '£8.00' '£2.99' '£4.49' '£5.49'\n",
      " '£4.35' '£2.75' '£3.69' '£6.49' '£9.49' '£7.99' '£1.29' '£2.19' '£2.69'\n",
      " '£0.39' '£2.00' '£1.69' '£1.39' '£1.99' '£3.25' '£4.29' '£4.69' '£3.49'\n",
      " '£10.99' '£1.15' '£35.00' '£38.00' '£18.00' '£32.00' '£14.00' '£6.00'\n",
      " '£17.00' '£16.00' '£5.00' '£40.00' '£65.00' '£70.00' '£80.00' '£60.00'\n",
      " '£99.00' '£110.00' '£100.00' '£9.00' '£8.99' 'XCD69KUI0K' '£55.00'\n",
      " '£46.00' '£34.00' '£85.00' '£9.99' 'N9D2BZQX63' '£24.00' '£28.00'\n",
      " 'ODPMASE7V7' '£49.00' '£59.00' '£1.00' '£1.49' '£0.79' '£1.70' '£1.20'\n",
      " '£1.35' '£1.25' '£2.49' '£0.49' '£0.69' '£0.89' '£3.29' '£4.79' '£1.89'\n",
      " '£3.39' '£0.50' '£2.79' '£4.25' '£0.75' '£0.85' '£1.10' '£26.64' '£22.23'\n",
      " '£7.49' '£69.00']\n",
      "Number of unique product prices: 133\n"
     ]
    }
   ],
   "source": [
    "# Get an overview of unique values in the product_price column\n",
    "unique_product_prices = df['product_price'].unique()\n",
    "print(\"Unique product prices:\", unique_product_prices)\n",
    "print(\"Number of unique product prices:\", len(unique_product_prices))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null product prices: 4\n"
     ]
    }
   ],
   "source": [
    "# Check for any null values in the product_price column\n",
    "null_product_prices = df['product_price'].isnull().sum()\n",
    "print(\"Number of null product prices:\", null_product_prices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of product prices without '£' symbol: 7\n",
      "Examples of product prices without '£' symbol:\n",
      " [nan 'XCD69KUI0K' 'N9D2BZQX63' 'ODPMASE7V7']\n"
     ]
    }
   ],
   "source": [
    "# Check for product prices that do not start with '£'\n",
    "non_pound_symbols = df['product_price'].apply(lambda x: not str(x).startswith('£'))\n",
    "print(\"Number of product prices without '£' symbol:\", non_pound_symbols.sum())\n",
    "print(\"Examples of product prices without '£' symbol:\\n\", df[non_pound_symbols]['product_price'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    toys-and-games\n",
      "1    toys-and-games\n",
      "2    toys-and-games\n",
      "3    toys-and-games\n",
      "4    toys-and-games\n",
      "Name: category, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the category column\n",
    "print(df['category'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique categories: ['toys-and-games' 'sports-and-leisure' nan 'pets' 'homeware' 'S1YB74MLMJ'\n",
      " 'C3NCA2CL35' 'WVPMHZP59U' 'health-and-beauty' 'food-and-drink' 'diy']\n",
      "Number of unique categories: 11\n"
     ]
    }
   ],
   "source": [
    "# Get unique values in the category column\n",
    "unique_categories = df['category'].unique()\n",
    "print(\"Unique categories:\", unique_categories)\n",
    "print(\"Number of unique categories:\", len(unique_categories))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of unique values in 'removed' column:\n",
      " removed\n",
      "Still_avaliable    1752\n",
      "Removed              94\n",
      "T3QRRH7SRP            1\n",
      "BPSADIOQOK            1\n",
      "H5N71TV8AY            1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get the count of each unique value in the removed column\n",
    "removed_counts = df['removed'].value_counts()\n",
    "print(\"Counts of unique values in 'removed' column:\\n\", removed_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the product_code column is treated as a string\n",
    "df['product_code'] = df['product_code'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regular expression pattern for the valid format\n",
    "pattern = r'^[A-Za-z0-9]{2}-[A-Za-z0-9]{8}$'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid product_code entries:\n",
      "10      g5-411324A\n",
      "19       w1-22434p\n",
      "25       w4-86578A\n",
      "28       k6-93543H\n",
      "38      z8-257244T\n",
      "           ...    \n",
      "1797    v0-697720L\n",
      "1805    U9-412268j\n",
      "1807    K6-678521h\n",
      "1816    b3-038131J\n",
      "1847    s1-001714d\n",
      "Name: product_code, Length: 195, dtype: object\n",
      "Number of invalid entries: 195\n"
     ]
    }
   ],
   "source": [
    "# Identify invalid product_code entries\n",
    "invalid_product_codes = df[~df['product_code'].str.match(pattern)]\n",
    "\n",
    "# Display the invalid entries\n",
    "print(\"Invalid product_code entries:\")\n",
    "print(invalid_product_codes['product_code'])\n",
    "print(\"Number of invalid entries:\", len(invalid_product_codes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning:\n",
      "   Unnamed: 0                                 product_name product_price  \\\n",
      "0           0  FurReal Dazzlin' Dimples My Playful Dolphin        £39.99   \n",
      "1           1          Tiffany's World Day Out At The Park        £12.99   \n",
      "2           2          Tiffany's World Pups Picnic Playset         £7.00   \n",
      "3           3     Tiffany's World Wildlife Park Adventures        £12.99   \n",
      "4           4                      Cosatto Cosy Dolls Pram        £30.00   \n",
      "\n",
      "   weight        category            EAN  date_added  \\\n",
      "0   1.6kg  toys-and-games  7425710935115  2005-12-02   \n",
      "1  0.48kg  toys-and-games   487128731892  2006-01-09   \n",
      "2    590g  toys-and-games  1945816904649  1997-03-29   \n",
      "3    540g  toys-and-games  1569790890899  2013-03-20   \n",
      "4  1.91kg  toys-and-games  7142740213920  2007-12-23   \n",
      "\n",
      "                                   uuid          removed product_code  \n",
      "0  83dc0a69-f96f-4c34-bcb7-928acae19a94  Still_avaliable  R7-3126933h  \n",
      "1  712254d7-aea7-4310-aff8-8bcdd0aec7ff  Still_avaliable  C2-7287916l  \n",
      "2  b089ef6f-b628-4e37-811d-fffe0102ba64  Still_avaliable  S7-1175877v  \n",
      "3  d55de422-8b98-47d6-9991-e4bc4c5c0cb0          Removed  D8-8421505n  \n",
      "4  7945b657-cb02-4cc5-96cf-f65ed0a8f235  Still_avaliable  B6-2596063a  \n"
     ]
    }
   ],
   "source": [
    "# Assuming the DataExtractor class is available in the same environment\n",
    "from data_extraction import DataExtractor\n",
    "\n",
    "# Create an instance of DataExtractor\n",
    "extractor = DataExtractor()\n",
    "\n",
    "# S3 URI\n",
    "s3_uri = 's3://data-handling-public/products.csv'\n",
    "\n",
    "# Retrieve data from S3\n",
    "df = extractor.extract_from_s3(s3_uri)\n",
    "\n",
    "# Display the first few rows of the dataframe to inspect it before cleaning\n",
    "print(\"Before cleaning:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning:\n",
      "   Unnamed: 0                                 product_name  product_price_gbp  \\\n",
      "0           0  FurReal Dazzlin' Dimples My Playful Dolphin              39.99   \n",
      "1           1          Tiffany's World Day Out At The Park              12.99   \n",
      "2           2          Tiffany's World Pups Picnic Playset               7.00   \n",
      "3           3     Tiffany's World Wildlife Park Adventures              12.99   \n",
      "4           4                      Cosatto Cosy Dolls Pram              30.00   \n",
      "\n",
      "         category            EAN  date_added  \\\n",
      "0  toys-and-games  7425710935115  2005-12-02   \n",
      "1  toys-and-games   487128731892  2006-01-09   \n",
      "2  toys-and-games  1945816904649  1997-03-29   \n",
      "3  toys-and-games  1569790890899  2013-03-20   \n",
      "4  toys-and-games  7142740213920  2007-12-23   \n",
      "\n",
      "                                   uuid          removed product_code  \\\n",
      "0  83dc0a69-f96f-4c34-bcb7-928acae19a94  Still_avaliable  R7-3126933h   \n",
      "1  712254d7-aea7-4310-aff8-8bcdd0aec7ff  Still_avaliable  C2-7287916l   \n",
      "2  b089ef6f-b628-4e37-811d-fffe0102ba64  Still_avaliable  S7-1175877v   \n",
      "3  d55de422-8b98-47d6-9991-e4bc4c5c0cb0          Removed  D8-8421505n   \n",
      "4  7945b657-cb02-4cc5-96cf-f65ed0a8f235  Still_avaliable  B6-2596063a   \n",
      "\n",
      "   weight_kg  \n",
      "0       1.60  \n",
      "1       0.48  \n",
      "2       0.59  \n",
      "3       0.54  \n",
      "4       1.91  \n"
     ]
    }
   ],
   "source": [
    "# Assuming the DataCleaning class is available in the same environment\n",
    "from data_cleaning import DataCleaning\n",
    "\n",
    "# Create an instance of DataCleaning\n",
    "cleaner = DataCleaning()\n",
    "\n",
    "# Apply the clean_product_data method to the dataframe\n",
    "df_cleaned = cleaner.clean_product_data(df)\n",
    "\n",
    "# Display the first few rows of the cleaned dataframe to inspect the changes\n",
    "print(\"After cleaning:\")\n",
    "print(df_cleaned.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data types before cleaning:\n",
      "Unnamed: 0             int64\n",
      "product_name          object\n",
      "product_price_gbp    float64\n",
      "category              object\n",
      "EAN                   object\n",
      "date_added            object\n",
      "uuid                  object\n",
      "removed               object\n",
      "product_code          object\n",
      "weight_kg            float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Display data types before cleaning\n",
    "print(\"\\nData types before cleaning:\")\n",
    "print(df_cleaned.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can only use .str accessor with string values!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Check for any non-numeric values in the product_price column\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m non_numeric_values \u001b[38;5;241m=\u001b[39m df_cleaned[\u001b[38;5;241m~\u001b[39m\u001b[43mdf_cleaned\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mproduct_price_gbp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr\u001b[49m\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39misnumeric()]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNon-numeric values in product_price column:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(non_numeric_values[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_price_gbp\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/AICore - Data Engineering/Data_Centralisation/venv/lib/python3.12/site-packages/pandas/core/generic.py:6299\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   6293\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[1;32m   6294\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[1;32m   6295\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[1;32m   6296\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   6297\u001b[0m ):\n\u001b[1;32m   6298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[0;32m-> 6299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/AICore - Data Engineering/Data_Centralisation/venv/lib/python3.12/site-packages/pandas/core/accessor.py:224\u001b[0m, in \u001b[0;36mCachedAccessor.__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessor\n\u001b[0;32m--> 224\u001b[0m accessor_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# NDFrame\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, accessor_obj)\n",
      "File \u001b[0;32m~/Documents/AICore - Data Engineering/Data_Centralisation/venv/lib/python3.12/site-packages/pandas/core/strings/accessor.py:191\u001b[0m, in \u001b[0;36mStringMethods.__init__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstring_\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StringDtype\n\u001b[0;32m--> 191\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_categorical \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(data\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype)\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(data\u001b[38;5;241m.\u001b[39mdtype, StringDtype)\n",
      "File \u001b[0;32m~/Documents/AICore - Data Engineering/Data_Centralisation/venv/lib/python3.12/site-packages/pandas/core/strings/accessor.py:245\u001b[0m, in \u001b[0;36mStringMethods._validate\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    242\u001b[0m inferred_dtype \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39minfer_dtype(values, skipna\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inferred_dtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m allowed_types:\n\u001b[0;32m--> 245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan only use .str accessor with string values!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inferred_dtype\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can only use .str accessor with string values!"
     ]
    }
   ],
   "source": [
    "# Check for any non-numeric values in the product_price column\n",
    "non_numeric_values = df_cleaned[~df_cleaned['product_price_gbp'].str.replace('.', '').str.isnumeric()]\n",
    "print(\"Non-numeric values in product_price column:\")\n",
    "print(non_numeric_values['product_price_gbp'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all entries in the product_price column are strings\n",
    "df['product_price'] = df['product_price'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric values in product_price column:\n",
      "0       £39.99\n",
      "1       £12.99\n",
      "2        £7.00\n",
      "3       £12.99\n",
      "4       £30.00\n",
      "         ...  \n",
      "1848    £15.00\n",
      "1849    £15.00\n",
      "1850    £18.00\n",
      "1851    £15.00\n",
      "1852    £69.00\n",
      "Name: product_price, Length: 1853, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check for any non-numeric values in the product_price column\n",
    "non_numeric_values = df[~df['product_price'].str.replace('.', '', regex=False).str.isnumeric()]\n",
    "print(\"Non-numeric values in product_price column:\")\n",
    "print(non_numeric_values['product_price'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
